{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ethereum-------------------\n",
      "bitcoin 5578\n",
      "ethereum 3972\n",
      "bitcoin ethereum 1621\n",
      "lift(bitcoin,ethereum) 0.8399185116901133318673525723\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 212\n",
      "bitcoin fraud 68\n",
      "lift(bitcoin,fraud) 0.6601405792297232388696834600\n",
      "-------------------bitcoin and blockchain-------------------\n",
      "bitcoin 5578\n",
      "blockchain 3837\n",
      "bitcoin blockchain 1329\n",
      "lift(bitcoin,blockchain) 0.7128473835135295003183230445\n",
      "-------------------bitcoin and ico-------------------\n",
      "bitcoin 5578\n",
      "ico 3472\n",
      "bitcoin ico 1510\n",
      "lift(bitcoin,ico) 0.8950774355474849350559224604\n",
      "-------------------bitcoin and job-------------------\n",
      "bitcoin 5578\n",
      "job 766\n",
      "bitcoin job 349\n",
      "lift(bitcoin,job) 0.9376916214108578366896432928\n",
      "-------------------ethereum and fraud-------------------\n",
      "ethereum 3972\n",
      "fraud 212\n",
      "ethereum fraud 108\n",
      "lift(ethereum,fraud) 1.472382146725189534287180072\n",
      "-------------------ethereum and blockchain-------------------\n",
      "ethereum 3972\n",
      "blockchain 3837\n",
      "ethereum blockchain 1517\n",
      "lift(ethereum,blockchain) 1.142684745787623082715311586\n",
      "-------------------ethereum and ico-------------------\n",
      "ethereum 3972\n",
      "ico 3472\n",
      "ethereum ico 1863\n",
      "lift(ethereum,ico) 1.550835688529383101062274632\n",
      "-------------------ethereum and job-------------------\n",
      "ethereum 3972\n",
      "job 766\n",
      "ethereum job 342\n",
      "lift(ethereum,job) 1.290416729114243569214264867\n",
      "-------------------fraud and blockchain-------------------\n",
      "fraud 212\n",
      "blockchain 3837\n",
      "fraud blockchain 114\n",
      "lift(fraud,blockchain) 1.608863056338235944945195982\n",
      "-------------------fraud and ico-------------------\n",
      "fraud 212\n",
      "ico 3472\n",
      "fraud ico 34\n",
      "lift(fraud,ico) 0.5302799756542909312233718807\n",
      "-------------------fraud and job-------------------\n",
      "fraud 212\n",
      "job 766\n",
      "fraud job 7\n",
      "lift(fraud,job) 0.4948519631508941327158973348\n",
      "-------------------blockchain and ico-------------------\n",
      "blockchain 3837\n",
      "ico 3472\n",
      "blockchain ico 1214\n",
      "lift(blockchain,ico) 1.046138196003261957006061523\n",
      "-------------------blockchain and job-------------------\n",
      "blockchain 3837\n",
      "job 766\n",
      "blockchain job 272\n",
      "lift(blockchain,job) 1.062405286985113342601344202\n",
      "-------------------ico and job-------------------\n",
      "ico 3472\n",
      "job 766\n",
      "ico job 395\n",
      "lift(ico,job) 1.705024004042786153457424408\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on usa_replace.csv\n"
     ]
    }
   ],
   "source": [
    "#created by Jonathan Malott (jm72636) on Sep 19 2016\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "#CSVs in a csv directory\n",
    "macros = ['usa_replace.csv']\n",
    "\n",
    "for x in macros:\n",
    "    filename = 'tweets_alldata.csv'\n",
    "    tempfile = NamedTemporaryFile(delete=False)\n",
    "\n",
    "    with open(filename, 'rb') as csvFile, tempfile:\n",
    "        reader = csv.reader(csvFile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            #this item is the forum post\n",
    "            item = row[2]\n",
    "\n",
    "            with open('csv/'+x, 'rb') as csvfile:\n",
    "                read = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                for row2 in read:\n",
    "                        #replace\n",
    "                        row[2] = row[2].lower().replace(row2[1].lower(),row2[0].lower())\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    shutil.move(tempfile.name, filename)\n",
    "    print \"Working on \"+x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:33: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written to word_freq.csv and word_pair_freq.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "sentences_all = []\n",
    "sentences_clean = []\n",
    "sentences_unpun = []\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\n",
    "for row in inter1:\n",
    "    sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "\n",
    "    for s in sentences:\n",
    "        in1 = ''.join(s)\n",
    "        out = re.sub('[%s]' % re.escape(string.punctuation), '', in1.lower())\n",
    "        sentences_all.append(out)\n",
    "\n",
    "for sentence in sentences_all:\n",
    "    s = []\n",
    "    for i in sentence.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    sentences_clean.append(s)\n",
    "\n",
    "for sentence in sentences_clean:\n",
    "    #print sentence\n",
    "    for word in sentence:\n",
    "        dictionary1[word] = 0\n",
    "\n",
    "for sentence in sentences_clean:\n",
    "    for word in sentence:\n",
    "        dictionary1[word] = dictionary1[word] + 1\n",
    "\n",
    "for sentence in sentences_clean:\n",
    "    for word in sentence:\n",
    "        for word2 in sentence:\n",
    "            if(word != word2):\n",
    "                d2_dict[word][word2] = 0\n",
    "\n",
    "for sentence in sentences_clean:\n",
    "    for word in sentence:\n",
    "        for word2 in sentence:\n",
    "            if(word != word2):\n",
    "                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "\n",
    "writer = csv.writer(open('word_freq_tweet1.csv', 'wb'))\n",
    "for key, value in dictionary1.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "writer = csv.writer(open('word_pair_freq_tweet1`.csv', 'wb'))\n",
    "for key1, value1 in d2_dict.items():\n",
    "    for key2, value2 in d2_dict[key1].items():\n",
    "        writer.writerow([key1, key2, value2])\n",
    "print \"written to word_freq.csv and word_pair_freq.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ethereum-------------------\n",
      "bitcoin 5578\n",
      "ethereum 3972\n",
      "bitcoin ethereum 1621\n",
      "lift(bitcoin,ethereum) 0.8399185116901133318673525723\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 408\n",
      "bitcoin fraud 174\n",
      "lift(bitcoin,fraud) 0.8777128635606268323033767110\n",
      "-------------------ethereum and fraud-------------------\n",
      "ethereum 3972\n",
      "fraud 408\n",
      "ethereum fraud 182\n",
      "lift(ethereum,fraud) 1.289269987954899986177754082\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on usa_replace.csv\n"
     ]
    }
   ],
   "source": [
    "#created by Jonathan Malott (jm72636) on Sep 19 2016\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "#CSVs in a csv directory\n",
    "macros = ['usa_replace.csv']\n",
    "\n",
    "for x in macros:\n",
    "    filename = 'tweets_alldata.csv'\n",
    "    tempfile = NamedTemporaryFile(delete=False)\n",
    "\n",
    "    with open(filename, 'rb') as csvFile, tempfile:\n",
    "        reader = csv.reader(csvFile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            #this item is the forum post\n",
    "            item = row[2]\n",
    "\n",
    "            with open('csv/'+x, 'rb') as csvfile:\n",
    "                read = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                for row2 in read:\n",
    "                        #replace\n",
    "                        row[2] = row[2].lower().replace(row2[1].lower(),row2[0].lower())\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    shutil.move(tempfile.name, filename)\n",
    "    print \"Working on \"+x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ethereum-------------------\n",
      "bitcoin 5578\n",
      "ethereum 3972\n",
      "bitcoin ethereum 1621\n",
      "lift(bitcoin,ethereum) 0.8399185116901133318673525723\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 487\n",
      "bitcoin fraud 235\n",
      "lift(bitcoin,fraud) 0.9931212603341228336902895874\n",
      "-------------------ethereum and fraud-------------------\n",
      "ethereum 3972\n",
      "fraud 487\n",
      "ethereum fraud 188\n",
      "lift(ethereum,fraud) 1.115736231650299529974710034\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ico-------------------\n",
      "bitcoin 5578\n",
      "ico 3472\n",
      "bitcoin ico 1510\n",
      "lift(bitcoin,ico) 0.8950774355474849350559224604\n",
      "-------------------bitcoin and blockchain-------------------\n",
      "bitcoin 5578\n",
      "blockchain 3837\n",
      "bitcoin blockchain 1329\n",
      "lift(bitcoin,blockchain) 0.7128473835135295003183230445\n",
      "-------------------bitcoin and job-------------------\n",
      "bitcoin 5578\n",
      "job 766\n",
      "bitcoin job 349\n",
      "lift(bitcoin,job) 0.9376916214108578366896432928\n",
      "-------------------bitcoin and china-------------------\n",
      "bitcoin 5578\n",
      "china 78\n",
      "bitcoin china 54\n",
      "lift(bitcoin,china) 1.424828309016189977107868825\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 487\n",
      "bitcoin fraud 235\n",
      "lift(bitcoin,fraud) 0.9931212603341228336902895874\n",
      "-------------------ico and blockchain-------------------\n",
      "ico 3472\n",
      "blockchain 3837\n",
      "ico blockchain 1214\n",
      "lift(ico,blockchain) 1.046138196003261957006061523\n",
      "-------------------ico and job-------------------\n",
      "ico 3472\n",
      "job 766\n",
      "ico job 395\n",
      "lift(ico,job) 1.705024004042786153457424408\n",
      "-------------------ico and china-------------------\n",
      "ico 3472\n",
      "china 78\n",
      "ico china 24\n",
      "lift(ico,china) 1.017369727047146401985111663\n",
      "-------------------ico and fraud-------------------\n",
      "ico 3472\n",
      "fraud 487\n",
      "ico fraud 65\n",
      "lift(ico,fraud) 0.4413128436113135059945684573\n",
      "-------------------blockchain and job-------------------\n",
      "blockchain 3837\n",
      "job 766\n",
      "blockchain job 272\n",
      "lift(blockchain,job) 1.062405286985113342601344202\n",
      "-------------------blockchain and china-------------------\n",
      "blockchain 3837\n",
      "china 78\n",
      "blockchain china 14\n",
      "lift(blockchain,china) 0.5370114205141570270577307325\n",
      "-------------------blockchain and fraud-------------------\n",
      "blockchain 3837\n",
      "fraud 487\n",
      "blockchain fraud 298\n",
      "lift(blockchain,fraud) 1.830785194841752117472850271\n",
      "-------------------job and china-------------------\n",
      "job 766\n",
      "china 78\n",
      "job china 6\n",
      "lift(job,china) 1.152841936131753364129343242\n",
      "-------------------job and fraud-------------------\n",
      "job 766\n",
      "fraud 487\n",
      "job fraud 16\n",
      "lift(job,fraud) 0.4923842355552457900182821237\n",
      "-------------------china and fraud-------------------\n",
      "china 78\n",
      "fraud 487\n",
      "china fraud 1\n",
      "lift(china,fraud) 0.3022166061180434897067340599\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ico-------------------\n",
      "bitcoin 5578\n",
      "ico 3472\n",
      "bitcoin ico 1510\n",
      "lift(bitcoin,ico) 0.8950774355474849350559224604\n",
      "-------------------bitcoin and blockchain-------------------\n",
      "bitcoin 5578\n",
      "blockchain 3837\n",
      "bitcoin blockchain 1329\n",
      "lift(bitcoin,blockchain) 0.7128473835135295003183230445\n",
      "-------------------bitcoin and job-------------------\n",
      "bitcoin 5578\n",
      "job 766\n",
      "bitcoin job 349\n",
      "lift(bitcoin,job) 0.9376916214108578366896432928\n",
      "-------------------bitcoin and china-------------------\n",
      "bitcoin 5578\n",
      "china 78\n",
      "bitcoin china 54\n",
      "lift(bitcoin,china) 1.424828309016189977107868825\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 487\n",
      "bitcoin fraud 235\n",
      "lift(bitcoin,fraud) 0.9931212603341228336902895874\n",
      "-------------------bitcoin and fintech-------------------\n",
      "bitcoin 5578\n",
      "fintech 818\n",
      "bitcoin fintech 505\n",
      "lift(bitcoin,fintech) 1.270578354888792067334034072\n",
      "-------------------bitcoin and change-------------------\n",
      "bitcoin 5578\n",
      "change 137\n",
      "bitcoin change 48\n",
      "lift(bitcoin,change) 0.7210809933707238813587268021\n",
      "-------------------bitcoin and thinking-------------------\n",
      "bitcoin 5578\n",
      "thinking 134\n",
      "bitcoin thinking 132\n",
      "lift(bitcoin,thinking) 2.027367643674777778372390468\n",
      "-------------------ico and blockchain-------------------\n",
      "ico 3472\n",
      "blockchain 3837\n",
      "ico blockchain 1214\n",
      "lift(ico,blockchain) 1.046138196003261957006061523\n",
      "-------------------ico and job-------------------\n",
      "ico 3472\n",
      "job 766\n",
      "ico job 395\n",
      "lift(ico,job) 1.705024004042786153457424408\n",
      "-------------------ico and china-------------------\n",
      "ico 3472\n",
      "china 78\n",
      "ico china 24\n",
      "lift(ico,china) 1.017369727047146401985111663\n",
      "-------------------ico and fraud-------------------\n",
      "ico 3472\n",
      "fraud 487\n",
      "ico fraud 65\n",
      "lift(ico,fraud) 0.4413128436113135059945684573\n",
      "-------------------ico and fintech-------------------\n",
      "ico 3472\n",
      "fintech 818\n",
      "ico fintech 191\n",
      "lift(ico,fintech) 0.7720443252622446565186528906\n",
      "-------------------ico and change-------------------\n",
      "ico 3472\n",
      "change 137\n",
      "ico change 88\n",
      "lift(ico,change) 2.123852130915940663998116317\n",
      "-------------------ico and thinking-------------------\n",
      "ico 3472\n",
      "thinking 134\n",
      "ico thinking 2\n",
      "lift(ico,thinking) 0.04935002407318247472315840154\n",
      "-------------------blockchain and job-------------------\n",
      "blockchain 3837\n",
      "job 766\n",
      "blockchain job 272\n",
      "lift(blockchain,job) 1.062405286985113342601344202\n",
      "-------------------blockchain and china-------------------\n",
      "blockchain 3837\n",
      "china 78\n",
      "blockchain china 14\n",
      "lift(blockchain,china) 0.5370114205141570270577307325\n",
      "-------------------blockchain and fraud-------------------\n",
      "blockchain 3837\n",
      "fraud 487\n",
      "blockchain fraud 298\n",
      "lift(blockchain,fraud) 1.830785194841752117472850271\n",
      "-------------------blockchain and fintech-------------------\n",
      "blockchain 3837\n",
      "fintech 818\n",
      "blockchain fintech 678\n",
      "lift(blockchain,fintech) 2.479856091728141828407355227\n",
      "-------------------blockchain and change-------------------\n",
      "blockchain 3837\n",
      "change 137\n",
      "blockchain change 48\n",
      "lift(blockchain,change) 1.048264211889991610690377405\n",
      "-------------------blockchain and thinking-------------------\n",
      "blockchain 3837\n",
      "thinking 134\n",
      "blockchain thinking 1\n",
      "lift(blockchain,thinking) 0.02232776695101505762819989186\n",
      "-------------------job and china-------------------\n",
      "job 766\n",
      "china 78\n",
      "job china 6\n",
      "lift(job,china) 1.152841936131753364129343242\n",
      "-------------------job and fraud-------------------\n",
      "job 766\n",
      "fraud 487\n",
      "job fraud 16\n",
      "lift(job,fraud) 0.4923842355552457900182821237\n",
      "-------------------job and fintech-------------------\n",
      "job 766\n",
      "fintech 818\n",
      "job fintech 64\n",
      "lift(job,fintech) 1.172572727214692908258696304\n",
      "-------------------job and change-------------------\n",
      "job 766\n",
      "change 137\n",
      "job change 41\n",
      "lift(job,change) 4.485144174877551409349926626\n",
      "-------------------job and thinking-------------------\n",
      "job 766\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------china and fraud-------------------\n",
      "china 78\n",
      "fraud 487\n",
      "china fraud 1\n",
      "lift(china,fraud) 0.3022166061180434897067340599\n",
      "-------------------china and fintech-------------------\n",
      "china 78\n",
      "fintech 818\n",
      "These words are not present together in a post\n",
      "-------------------china and change-------------------\n",
      "china 78\n",
      "change 137\n",
      "These words are not present together in a post\n",
      "-------------------china and thinking-------------------\n",
      "china 78\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------fraud and fintech-------------------\n",
      "fraud 487\n",
      "fintech 818\n",
      "fraud fintech 136\n",
      "lift(fraud,fintech) 3.919209972738637333507377638\n",
      "-------------------fraud and change-------------------\n",
      "fraud 487\n",
      "change 137\n",
      "fraud change 2\n",
      "lift(fraud,change) 0.3441298580614217838996387836\n",
      "-------------------fraud and thinking-------------------\n",
      "fraud 487\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------fintech and change-------------------\n",
      "fintech 818\n",
      "change 137\n",
      "fintech change 1\n",
      "lift(fintech,change) 0.1024396337872325236913961416\n",
      "-------------------fintech and thinking-------------------\n",
      "fintech 818\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------change and thinking-------------------\n",
      "change 137\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on usa_replace.csv\n"
     ]
    }
   ],
   "source": [
    "#created by Jonathan Malott (jm72636) on Sep 19 2016\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "#CSVs in a csv directory\n",
    "macros = ['usa_replace.csv']\n",
    "\n",
    "for x in macros:\n",
    "    filename = 'tweets_alldata.csv'\n",
    "    tempfile = NamedTemporaryFile(delete=False)\n",
    "\n",
    "    with open(filename, 'rb') as csvFile, tempfile:\n",
    "        reader = csv.reader(csvFile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            #this item is the forum post\n",
    "            item = row[2]\n",
    "\n",
    "            with open('csv/'+x, 'rb') as csvfile:\n",
    "                read = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                for row2 in read:\n",
    "                        #replace\n",
    "                        row[2] = row[2].lower().replace(row2[1].lower(),row2[0].lower())\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    shutil.move(tempfile.name, filename)\n",
    "    print \"Working on \"+x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ethereum-------------------\n",
      "bitcoin 5578\n",
      "ethereum 3972\n",
      "bitcoin ethereum 1621\n",
      "lift(bitcoin,ethereum) 0.8399185116901133318673525723\n",
      "-------------------bitcoin and ico-------------------\n",
      "bitcoin 5578\n",
      "ico 3472\n",
      "bitcoin ico 1510\n",
      "lift(bitcoin,ico) 0.8950774355474849350559224604\n",
      "-------------------bitcoin and blockchain-------------------\n",
      "bitcoin 5578\n",
      "blockchain 3837\n",
      "bitcoin blockchain 1329\n",
      "lift(bitcoin,blockchain) 0.7128473835135295003183230445\n",
      "-------------------bitcoin and job-------------------\n",
      "bitcoin 5578\n",
      "job 766\n",
      "bitcoin job 349\n",
      "lift(bitcoin,job) 0.9376916214108578366896432928\n",
      "-------------------bitcoin and china-------------------\n",
      "bitcoin 5578\n",
      "china 78\n",
      "bitcoin china 54\n",
      "lift(bitcoin,china) 1.424828309016189977107868825\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 559\n",
      "bitcoin fraud 248\n",
      "lift(bitcoin,fraud) 0.9130682703773000370096937175\n",
      "-------------------bitcoin and fintech-------------------\n",
      "bitcoin 5578\n",
      "fintech 818\n",
      "bitcoin fintech 505\n",
      "lift(bitcoin,fintech) 1.270578354888792067334034072\n",
      "-------------------bitcoin and thinking-------------------\n",
      "bitcoin 5578\n",
      "thinking 134\n",
      "bitcoin thinking 132\n",
      "lift(bitcoin,thinking) 2.027367643674777778372390468\n",
      "-------------------bitcoin and invest-------------------\n",
      "bitcoin 5578\n",
      "invest 207\n",
      "bitcoin invest 125\n",
      "lift(bitcoin,invest) 1.242805154133821101878844252\n",
      "-------------------ethereum and ico-------------------\n",
      "ethereum 3972\n",
      "ico 3472\n",
      "ethereum ico 1863\n",
      "lift(ethereum,ico) 1.550835688529383101062274632\n",
      "-------------------ethereum and blockchain-------------------\n",
      "ethereum 3972\n",
      "blockchain 3837\n",
      "ethereum blockchain 1517\n",
      "lift(ethereum,blockchain) 1.142684745787623082715311586\n",
      "-------------------ethereum and job-------------------\n",
      "ethereum 3972\n",
      "job 766\n",
      "ethereum job 342\n",
      "lift(ethereum,job) 1.290416729114243569214264867\n",
      "-------------------ethereum and china-------------------\n",
      "ethereum 3972\n",
      "china 78\n",
      "ethereum china 11\n",
      "lift(ethereum,china) 0.4075967671133834275828233532\n",
      "-------------------ethereum and fraud-------------------\n",
      "ethereum 3972\n",
      "fraud 559\n",
      "ethereum fraud 200\n",
      "lift(ethereum,fraud) 1.034072136439873389216465167\n",
      "-------------------ethereum and fintech-------------------\n",
      "ethereum 3972\n",
      "fintech 818\n",
      "ethereum fintech 353\n",
      "lift(ethereum,fintech) 1.247251543198477361087514804\n",
      "-------------------ethereum and thinking-------------------\n",
      "ethereum 3972\n",
      "thinking 134\n",
      "ethereum thinking 1\n",
      "lift(ethereum,thinking) 0.02156889269663765763328373240\n",
      "-------------------ethereum and invest-------------------\n",
      "ethereum 3972\n",
      "invest 207\n",
      "ethereum invest 51\n",
      "lift(ethereum,invest) 0.7120860516368200592553672811\n",
      "-------------------ico and blockchain-------------------\n",
      "ico 3472\n",
      "blockchain 3837\n",
      "ico blockchain 1214\n",
      "lift(ico,blockchain) 1.046138196003261957006061523\n",
      "-------------------ico and job-------------------\n",
      "ico 3472\n",
      "job 766\n",
      "ico job 395\n",
      "lift(ico,job) 1.705024004042786153457424408\n",
      "-------------------ico and china-------------------\n",
      "ico 3472\n",
      "china 78\n",
      "ico china 24\n",
      "lift(ico,china) 1.017369727047146401985111663\n",
      "-------------------ico and fraud-------------------\n",
      "ico 3472\n",
      "fraud 559\n",
      "ico fraud 112\n",
      "lift(ico,fraud) 0.6624733106353511454786773616\n",
      "-------------------ico and fintech-------------------\n",
      "ico 3472\n",
      "fintech 818\n",
      "ico fintech 191\n",
      "lift(ico,fintech) 0.7720443252622446565186528906\n",
      "-------------------ico and thinking-------------------\n",
      "ico 3472\n",
      "thinking 134\n",
      "ico thinking 2\n",
      "lift(ico,thinking) 0.04935002407318247472315840154\n",
      "-------------------ico and invest-------------------\n",
      "ico 3472\n",
      "invest 207\n",
      "ico invest 60\n",
      "lift(ico,invest) 0.9583917718560074801309022908\n",
      "-------------------blockchain and job-------------------\n",
      "blockchain 3837\n",
      "job 766\n",
      "blockchain job 272\n",
      "lift(blockchain,job) 1.062405286985113342601344202\n",
      "-------------------blockchain and china-------------------\n",
      "blockchain 3837\n",
      "china 78\n",
      "blockchain china 14\n",
      "lift(blockchain,china) 0.5370114205141570270577307325\n",
      "-------------------blockchain and fraud-------------------\n",
      "blockchain 3837\n",
      "fraud 559\n",
      "blockchain fraud 328\n",
      "lift(blockchain,fraud) 1.755545640484818985464475218\n",
      "-------------------blockchain and fintech-------------------\n",
      "blockchain 3837\n",
      "fintech 818\n",
      "blockchain fintech 678\n",
      "lift(blockchain,fintech) 2.479856091728141828407355227\n",
      "-------------------blockchain and thinking-------------------\n",
      "blockchain 3837\n",
      "thinking 134\n",
      "blockchain thinking 1\n",
      "lift(blockchain,thinking) 0.02232776695101505762819989186\n",
      "-------------------blockchain and invest-------------------\n",
      "blockchain 3837\n",
      "invest 207\n",
      "blockchain invest 52\n",
      "lift(blockchain,invest) 0.7515936237423812635424968430\n",
      "-------------------job and china-------------------\n",
      "job 766\n",
      "china 78\n",
      "job china 6\n",
      "lift(job,china) 1.152841936131753364129343242\n",
      "-------------------job and fraud-------------------\n",
      "job 766\n",
      "fraud 559\n",
      "job fraud 16\n",
      "lift(job,fraud) 0.4289644413513500889783602759\n",
      "-------------------job and fintech-------------------\n",
      "job 766\n",
      "fintech 818\n",
      "job fintech 64\n",
      "lift(job,fintech) 1.172572727214692908258696304\n",
      "-------------------job and thinking-------------------\n",
      "job 766\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------job and invest-------------------\n",
      "job 766\n",
      "invest 207\n",
      "job invest 10\n",
      "lift(job,invest) 0.7240070130296035620135972049\n",
      "-------------------china and fraud-------------------\n",
      "china 78\n",
      "fraud 559\n",
      "china fraud 1\n",
      "lift(china,fraud) 0.2632906747396908398697307463\n",
      "-------------------china and fintech-------------------\n",
      "china 78\n",
      "fintech 818\n",
      "These words are not present together in a post\n",
      "-------------------china and thinking-------------------\n",
      "china 78\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------china and invest-------------------\n",
      "china 78\n",
      "invest 207\n",
      "These words are not present together in a post\n",
      "-------------------fraud and fintech-------------------\n",
      "fraud 559\n",
      "fintech 818\n",
      "fraud fintech 136\n",
      "lift(fraud,fintech) 3.414410119362641111660273541\n",
      "-------------------fraud and thinking-------------------\n",
      "fraud 559\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------fraud and invest-------------------\n",
      "fraud 559\n",
      "invest 207\n",
      "fraud invest 11\n",
      "lift(fraud,invest) 1.091320767761617104387579615\n",
      "-------------------fintech and thinking-------------------\n",
      "fintech 818\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------fintech and invest-------------------\n",
      "fintech 818\n",
      "invest 207\n",
      "fintech invest 5\n",
      "lift(fintech,invest) 0.3389910586678950663217698404\n",
      "-------------------thinking and invest-------------------\n",
      "thinking 134\n",
      "invest 207\n",
      "These words are not present together in a post\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on usa_replace.csv\n"
     ]
    }
   ],
   "source": [
    "#created by Jonathan Malott (jm72636) on Sep 19 2016\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "#CSVs in a csv directory\n",
    "macros = ['usa_replace.csv']\n",
    "\n",
    "for x in macros:\n",
    "    filename = 'tweets_alldata.csv'\n",
    "    tempfile = NamedTemporaryFile(delete=False)\n",
    "\n",
    "    with open(filename, 'rb') as csvFile, tempfile:\n",
    "        reader = csv.reader(csvFile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            #this item is the forum post\n",
    "            item = row[2]\n",
    "\n",
    "            with open('csv/'+x, 'rb') as csvfile:\n",
    "                read = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                for row2 in read:\n",
    "                        #replace\n",
    "                        row[2] = row[2].lower().replace(row2[1].lower(),row2[0].lower())\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    shutil.move(tempfile.name, filename)\n",
    "    print \"Working on \"+x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ethereum-------------------\n",
      "bitcoin 5578\n",
      "ethereum 3972\n",
      "bitcoin ethereum 1621\n",
      "lift(bitcoin,ethereum) 0.8399185116901133318673525723\n",
      "-------------------bitcoin and china-------------------\n",
      "bitcoin 5578\n",
      "china 82\n",
      "bitcoin china 56\n",
      "lift(bitcoin,china) 1.405521692362854069558981714\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 559\n",
      "bitcoin fraud 248\n",
      "lift(bitcoin,fraud) 0.9130682703773000370096937175\n",
      "-------------------bitcoin and fintech-------------------\n",
      "bitcoin 5578\n",
      "fintech 818\n",
      "bitcoin fintech 505\n",
      "lift(bitcoin,fintech) 1.270578354888792067334034072\n",
      "-------------------bitcoin and thinking-------------------\n",
      "bitcoin 5578\n",
      "thinking 134\n",
      "bitcoin thinking 132\n",
      "lift(bitcoin,thinking) 2.027367643674777778372390468\n",
      "-------------------bitcoin and invest-------------------\n",
      "bitcoin 5578\n",
      "invest 207\n",
      "bitcoin invest 125\n",
      "lift(bitcoin,invest) 1.242805154133821101878844252\n",
      "-------------------ethereum and china-------------------\n",
      "ethereum 3972\n",
      "china 82\n",
      "ethereum china 11\n",
      "lift(ethereum,china) 0.3877139979859013091641490433\n",
      "-------------------ethereum and fraud-------------------\n",
      "ethereum 3972\n",
      "fraud 559\n",
      "ethereum fraud 200\n",
      "lift(ethereum,fraud) 1.034072136439873389216465167\n",
      "-------------------ethereum and fintech-------------------\n",
      "ethereum 3972\n",
      "fintech 818\n",
      "ethereum fintech 353\n",
      "lift(ethereum,fintech) 1.247251543198477361087514804\n",
      "-------------------ethereum and thinking-------------------\n",
      "ethereum 3972\n",
      "thinking 134\n",
      "ethereum thinking 1\n",
      "lift(ethereum,thinking) 0.02156889269663765763328373240\n",
      "-------------------ethereum and invest-------------------\n",
      "ethereum 3972\n",
      "invest 207\n",
      "ethereum invest 51\n",
      "lift(ethereum,invest) 0.7120860516368200592553672811\n",
      "-------------------china and fraud-------------------\n",
      "china 82\n",
      "fraud 559\n",
      "china fraud 1\n",
      "lift(china,fraud) 0.2504472271914132379248658318\n",
      "-------------------china and fintech-------------------\n",
      "china 82\n",
      "fintech 818\n",
      "These words are not present together in a post\n",
      "-------------------china and thinking-------------------\n",
      "china 82\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------china and invest-------------------\n",
      "china 82\n",
      "invest 207\n",
      "These words are not present together in a post\n",
      "-------------------fraud and fintech-------------------\n",
      "fraud 559\n",
      "fintech 818\n",
      "fraud fintech 136\n",
      "lift(fraud,fintech) 3.414410119362641111660273541\n",
      "-------------------fraud and thinking-------------------\n",
      "fraud 559\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------fraud and invest-------------------\n",
      "fraud 559\n",
      "invest 207\n",
      "fraud invest 11\n",
      "lift(fraud,invest) 1.091320767761617104387579615\n",
      "-------------------fintech and thinking-------------------\n",
      "fintech 818\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------fintech and invest-------------------\n",
      "fintech 818\n",
      "invest 207\n",
      "fintech invest 5\n",
      "lift(fintech,invest) 0.3389910586678950663217698404\n",
      "-------------------thinking and invest-------------------\n",
      "thinking 134\n",
      "invest 207\n",
      "These words are not present together in a post\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on usa_replace.csv\n"
     ]
    }
   ],
   "source": [
    "#created by Jonathan Malott (jm72636) on Sep 19 2016\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "#CSVs in a csv directory\n",
    "macros = ['usa_replace.csv']\n",
    "\n",
    "for x in macros:\n",
    "    filename = 'tweets_alldata.csv'\n",
    "    tempfile = NamedTemporaryFile(delete=False)\n",
    "\n",
    "    with open(filename, 'rb') as csvFile, tempfile:\n",
    "        reader = csv.reader(csvFile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            #this item is the forum post\n",
    "            item = row[2]\n",
    "\n",
    "            with open('csv/'+x, 'rb') as csvfile:\n",
    "                read = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                for row2 in read:\n",
    "                        #replace\n",
    "                        row[2] = row[2].lower().replace(row2[1].lower(),row2[0].lower())\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    shutil.move(tempfile.name, filename)\n",
    "    print \"Working on \"+x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ethereum-------------------\n",
      "bitcoin 5578\n",
      "ethereum 3972\n",
      "bitcoin ethereum 1621\n",
      "lift(bitcoin,ethereum) 0.8399185116901133318673525723\n",
      "-------------------bitcoin and china-------------------\n",
      "bitcoin 5578\n",
      "china 119\n",
      "bitcoin china 82\n",
      "lift(bitcoin,china) 1.418176449496973403918756459\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 559\n",
      "bitcoin fraud 248\n",
      "lift(bitcoin,fraud) 0.9130682703773000370096937175\n",
      "-------------------bitcoin and fintech-------------------\n",
      "bitcoin 5578\n",
      "fintech 818\n",
      "bitcoin fintech 505\n",
      "lift(bitcoin,fintech) 1.270578354888792067334034072\n",
      "-------------------bitcoin and thinking-------------------\n",
      "bitcoin 5578\n",
      "thinking 134\n",
      "bitcoin thinking 132\n",
      "lift(bitcoin,thinking) 2.027367643674777778372390468\n",
      "-------------------bitcoin and invest-------------------\n",
      "bitcoin 5578\n",
      "invest 207\n",
      "bitcoin invest 125\n",
      "lift(bitcoin,invest) 1.242805154133821101878844252\n",
      "-------------------ethereum and china-------------------\n",
      "ethereum 3972\n",
      "china 119\n",
      "ethereum china 17\n",
      "lift(ethereum,china) 0.4128902316213494461228600201\n",
      "-------------------ethereum and fraud-------------------\n",
      "ethereum 3972\n",
      "fraud 559\n",
      "ethereum fraud 200\n",
      "lift(ethereum,fraud) 1.034072136439873389216465167\n",
      "-------------------ethereum and fintech-------------------\n",
      "ethereum 3972\n",
      "fintech 818\n",
      "ethereum fintech 353\n",
      "lift(ethereum,fintech) 1.247251543198477361087514804\n",
      "-------------------ethereum and thinking-------------------\n",
      "ethereum 3972\n",
      "thinking 134\n",
      "ethereum thinking 1\n",
      "lift(ethereum,thinking) 0.02156889269663765763328373240\n",
      "-------------------ethereum and invest-------------------\n",
      "ethereum 3972\n",
      "invest 207\n",
      "ethereum invest 51\n",
      "lift(ethereum,invest) 0.7120860516368200592553672811\n",
      "-------------------china and fraud-------------------\n",
      "china 119\n",
      "fraud 559\n",
      "china fraud 4\n",
      "lift(china,fraud) 0.6903083236872566557929075029\n",
      "-------------------china and fintech-------------------\n",
      "china 119\n",
      "fintech 818\n",
      "These words are not present together in a post\n",
      "-------------------china and thinking-------------------\n",
      "china 119\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------china and invest-------------------\n",
      "china 119\n",
      "invest 207\n",
      "These words are not present together in a post\n",
      "-------------------fraud and fintech-------------------\n",
      "fraud 559\n",
      "fintech 818\n",
      "fraud fintech 136\n",
      "lift(fraud,fintech) 3.414410119362641111660273541\n",
      "-------------------fraud and thinking-------------------\n",
      "fraud 559\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------fraud and invest-------------------\n",
      "fraud 559\n",
      "invest 207\n",
      "fraud invest 11\n",
      "lift(fraud,invest) 1.091320767761617104387579615\n",
      "-------------------fintech and thinking-------------------\n",
      "fintech 818\n",
      "thinking 134\n",
      "These words are not present together in a post\n",
      "-------------------fintech and invest-------------------\n",
      "fintech 818\n",
      "invest 207\n",
      "fintech invest 5\n",
      "lift(fintech,invest) 0.3389910586678950663217698404\n",
      "-------------------thinking and invest-------------------\n",
      "thinking 134\n",
      "invest 207\n",
      "These words are not present together in a post\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on usa_replace.csv\n"
     ]
    }
   ],
   "source": [
    "#created by Jonathan Malott (jm72636) on Sep 19 2016\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "#CSVs in a csv directory\n",
    "macros = ['usa_replace.csv']\n",
    "\n",
    "for x in macros:\n",
    "    filename = 'tweets_alldata.csv'\n",
    "    tempfile = NamedTemporaryFile(delete=False)\n",
    "\n",
    "    with open(filename, 'rb') as csvFile, tempfile:\n",
    "        reader = csv.reader(csvFile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            #this item is the forum post\n",
    "            item = row[2]\n",
    "\n",
    "            with open('csv/'+x, 'rb') as csvfile:\n",
    "                read = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                for row2 in read:\n",
    "                        #replace\n",
    "                        row[2] = row[2].lower().replace(row2[1].lower(),row2[0].lower())\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    shutil.move(tempfile.name, filename)\n",
    "    print \"Working on \"+x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshatparakh/Desktop/Python/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\n",
      "-------------------bitcoin and ethereum-------------------\n",
      "bitcoin 5578\n",
      "ethereum 3972\n",
      "bitcoin ethereum 1621\n",
      "lift(bitcoin,ethereum) 0.8399185116901133318673525723\n",
      "-------------------bitcoin and china-------------------\n",
      "bitcoin 5578\n",
      "china 119\n",
      "bitcoin china 82\n",
      "lift(bitcoin,china) 1.418176449496973403918756459\n",
      "-------------------bitcoin and fraud-------------------\n",
      "bitcoin 5578\n",
      "fraud 559\n",
      "bitcoin fraud 248\n",
      "lift(bitcoin,fraud) 0.9130682703773000370096937175\n",
      "-------------------bitcoin and fintech-------------------\n",
      "bitcoin 5578\n",
      "fintech 818\n",
      "bitcoin fintech 505\n",
      "lift(bitcoin,fintech) 1.270578354888792067334034072\n",
      "-------------------bitcoin and thinking-------------------\n",
      "bitcoin 5578\n",
      "thinking 213\n",
      "bitcoin thinking 153\n",
      "lift(bitcoin,thinking) 1.478342987289098520849009438\n",
      "-------------------bitcoin and invest-------------------\n",
      "bitcoin 5578\n",
      "invest 207\n",
      "bitcoin invest 125\n",
      "lift(bitcoin,invest) 1.242805154133821101878844252\n",
      "-------------------ethereum and china-------------------\n",
      "ethereum 3972\n",
      "china 119\n",
      "ethereum china 17\n",
      "lift(ethereum,china) 0.4128902316213494461228600201\n",
      "-------------------ethereum and fraud-------------------\n",
      "ethereum 3972\n",
      "fraud 559\n",
      "ethereum fraud 200\n",
      "lift(ethereum,fraud) 1.034072136439873389216465167\n",
      "-------------------ethereum and fintech-------------------\n",
      "ethereum 3972\n",
      "fintech 818\n",
      "ethereum fintech 353\n",
      "lift(ethereum,fintech) 1.247251543198477361087514804\n",
      "-------------------ethereum and thinking-------------------\n",
      "ethereum 3972\n",
      "thinking 213\n",
      "ethereum thinking 21\n",
      "lift(ethereum,thinking) 0.2849524133724806036622555069\n",
      "-------------------ethereum and invest-------------------\n",
      "ethereum 3972\n",
      "invest 207\n",
      "ethereum invest 51\n",
      "lift(ethereum,invest) 0.7120860516368200592553672811\n",
      "-------------------china and fraud-------------------\n",
      "china 119\n",
      "fraud 559\n",
      "china fraud 4\n",
      "lift(china,fraud) 0.6903083236872566557929075029\n",
      "-------------------china and fintech-------------------\n",
      "china 119\n",
      "fintech 818\n",
      "These words are not present together in a post\n",
      "-------------------china and thinking-------------------\n",
      "china 119\n",
      "thinking 213\n",
      "These words are not present together in a post\n",
      "-------------------china and invest-------------------\n",
      "china 119\n",
      "invest 207\n",
      "These words are not present together in a post\n",
      "-------------------fraud and fintech-------------------\n",
      "fraud 559\n",
      "fintech 818\n",
      "fraud fintech 136\n",
      "lift(fraud,fintech) 3.414410119362641111660273541\n",
      "-------------------fraud and thinking-------------------\n",
      "fraud 559\n",
      "thinking 213\n",
      "fraud thinking 9\n",
      "lift(fraud,thinking) 0.8677467308322205145002393610\n",
      "-------------------fraud and invest-------------------\n",
      "fraud 559\n",
      "invest 207\n",
      "fraud invest 11\n",
      "lift(fraud,invest) 1.091320767761617104387579615\n",
      "-------------------fintech and thinking-------------------\n",
      "fintech 818\n",
      "thinking 213\n",
      "fintech thinking 3\n",
      "lift(fintech,thinking) 0.1976652088570543062777643858\n",
      "-------------------fintech and invest-------------------\n",
      "fintech 818\n",
      "invest 207\n",
      "fintech invest 5\n",
      "lift(fintech,invest) 0.3389910586678950663217698404\n",
      "-------------------thinking and invest-------------------\n",
      "thinking 213\n",
      "invest 207\n",
      "thinking invest 19\n",
      "lift(thinking,invest) 4.947041346306502460819668413\n",
      "--------------------------------------------------------\n",
      "Consolidated lift values can be found in Lift_Values.csv file\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import decimal\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "posts_all = []\n",
    "posts_clean = []\n",
    "keys_all = []           # Array to store all keys from the ---- edmunds_pair_keys1.txt ---- file\n",
    "file_length = 0\t\t# calculate the number of rows in the file\n",
    "results_dict = {'Pair':'Lift Value'};\n",
    "writer_output = csv.writer(open('Lift_Values.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_alldata.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "\t#file_length = '[%s]' % len(f.readlines())\n",
    "\t#print \"Hello\"\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\tfile_length = file_length + 1\n",
    "\n",
    "for row in inter1:\n",
    "    #sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    out1 = re.sub('[%s]' % re.escape(string.punctuation), '', row.lower())\n",
    "    posts_all.append(out1)\n",
    "\n",
    "for post in posts_all:\n",
    "    s = []\n",
    "    for i in post.split():\n",
    "        if i not in stop:\n",
    "            s.append(i)\n",
    "    posts_clean.append(s)\n",
    "\n",
    "print \"WARNING::EVERYTHING NEEDS TO BE IN LOWER CASE edmunds_pair_keys.txt file\"\n",
    "with open('edmunds_pair_keys.txt') as fileText:\n",
    "    for row in fileText:\n",
    "        keys_all = row.split(\",\")\n",
    "    #print keys_all\n",
    "    length = len(keys_all)\n",
    "    for index in range(len(keys_all)):\n",
    "        #print str(index) + '  ' + keys_all[index]\n",
    "        subs_counter = index + 1\n",
    "        while subs_counter < len(keys_all):\n",
    "            #print keys_all[index] + '    ' + keys_all[subs_counter]\n",
    "            #with open('edmunds_pair_keys.csv') as f:\n",
    "                #pair_key_rows = csv.DictReader(f, delimiter = ',')\n",
    "                #for row in pair_key_rows:\n",
    "                    #nb = raw_input('Choose a Word 1: ')\n",
    "                    #nb2 = raw_input('Choose the Word 2: ')\n",
    "\n",
    "                    #nb = row[\"car_maker1\"]\n",
    "                    #nb2 = row[\"car_maker2\"]\n",
    "            nb = keys_all[index]\n",
    "            nb2 = keys_all[subs_counter]\n",
    "            print '-------------------' + nb + ' and ' + nb2 + '-------------------'\n",
    "            subs_counter = subs_counter + 1\n",
    "\n",
    "            nb_plu = nb + 's'\n",
    "            nb_app = nb + \"'s\"\n",
    "\n",
    "            nb2_plu = nb2 + 's'\n",
    "            nb2_app = nb2 + \"'s\"\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for n,word in enumerate(post):\n",
    "                    if(word == nb_plu or word == nb_app):\n",
    "                        post[n] = nb\n",
    "                    elif(word == nb2_plu or word == nb2_app):\n",
    "                        post[n] = nb2\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    dictionary1[word] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                x = {}\n",
    "                for word in post:\n",
    "                    x[word] = 1\n",
    "                for word in post:\n",
    "                    if (x[word] > 0):\n",
    "                        dictionary1[word] = dictionary1[word] + 1\n",
    "                    x[word] = -1\n",
    "\n",
    "            writer = csv.writer(open('word_post.csv', 'wb'))\n",
    "            for key, value in dictionary1.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "            for post in posts_clean:\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d2_dict[word][word2] = 0\n",
    "\n",
    "            for post in posts_clean:\n",
    "                d3_dict = defaultdict(dict)\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            d3_dict[word][word2] = 1\n",
    "\n",
    "                for word in post:\n",
    "                    for word2 in post:\n",
    "                        if(word != word2):\n",
    "                            if (d3_dict[word][word2] > 0):\n",
    "                                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                            d3_dict[word][word2] = -1\n",
    "\n",
    "\n",
    "            if(dictionary1.has_key(nb)):\n",
    "                print nb + \" \" + str(dictionary1[nb])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(dictionary1.has_key(nb2)):\n",
    "                print nb2 + \" \" + str(dictionary1[nb2])\n",
    "            else:\n",
    "                print \"This word is not present\"\n",
    "                exit(0)\n",
    "\n",
    "            if(d2_dict.has_key(nb)):\n",
    "                if(d2_dict[nb].has_key(nb2)):\n",
    "                    print nb +\" \" +nb2 +\" \" + str(d2_dict[nb][nb2])\n",
    "                    #print 'Rows in file: ', file_length\n",
    "                    #if(d2_dict[nb].has_key(nb2)):\n",
    "                    #results_dict['Pair'] = nb + \"_\" +nb2\n",
    "                    #results_dict['Lift Value'] = decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    results_dict.update({nb + \"_\" +nb2:decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))})\n",
    "                    print 'lift('+ nb + \",\" +nb2 + ')',decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal((dictionary1[nb]*dictionary1[nb2])))\n",
    "                    #writer_output = csv.writer(open('Lift_Values.csv', 'wb'))\n",
    "                    #for key, value in results_dict.items():\n",
    "                    writer_output.writerow([nb + \"_\" +nb2, decimal.Decimal(decimal.Decimal(file_length*(d2_dict[nb][nb2]))/decimal.Decimal(dictionary1[nb]*dictionary1[nb2]))])\n",
    "                else:\n",
    "                    print \"These words are not present together in a post\"\n",
    "                    exit(0)\n",
    "            else:\n",
    "                print \"These words are not present together in a post\"\n",
    "                exit(0)\n",
    "print '--------------------------------------------------------'                \n",
    "print 'Consolidated lift values can be found in Lift_Values.csv file'            \n",
    "\n",
    "            #writer = csv.writer(open('word_pair_post.csv', 'wb'))\n",
    "            #for key1, value1 in d2_dict.items():\n",
    "            #    for key2, value2 in d2_dict[key1].items():\n",
    "            #        writer.writerow([key1, key2, value2])\n",
    "\n",
    "            #print \"written to word_post.csv and word_pair_post.csv\"\n",
    "#print results_dict.values()\n",
    "##writer = csv.writer(open('lift_values.csv', 'wb'),'wb')\n",
    "#for key, value in results_dict.items():\n",
    "   #writer.writerow([key, value])\n",
    "##writer.writeheader()\n",
    "#for row in results_dict:\n",
    "#    print row\n",
    "##for key,value in results_dict.iteritems():\n",
    " ##   writer.writerow([key, value])\n",
    "#for row in results_dict:\n",
    "#    writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import urllib2 as urllib\n",
    "import csv\n",
    "from csv import writer\n",
    "import json\n",
    "import time\n",
    "\n",
    "api_key = \"Your api key\"\n",
    "api_secret = \"Your api secret\"\n",
    "access_token_key = \"Your access token key\"\n",
    "access_token_secret = \"Your access token secret\"\n",
    "\n",
    "_debug=0\n",
    "\n",
    "oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)\n",
    "oauth_consumer = oauth.Consumer(key=api_key, secret=api_secret)\n",
    "\n",
    "signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()\n",
    "\n",
    "http_method = \"GET\"\n",
    "\n",
    "http_handler  = urllib.HTTPHandler(debuglevel=_debug)\n",
    "https_handler = urllib.HTTPSHandler(debuglevel=_debug)\n",
    "\n",
    "'''\n",
    "Construct, sign, and open a twitter request\n",
    "using the hard-coded credentials above.\n",
    "'''\n",
    "def twitterreq(url, method, parameters):\n",
    "  req = oauth.Request.from_consumer_and_token(oauth_consumer,\n",
    "                                             token=oauth_token,\n",
    "                                             http_method=http_method,\n",
    "                                             http_url=url, \n",
    "                                             parameters=parameters)\n",
    "\n",
    "  req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)\n",
    "\n",
    "  headers = req.to_header()\n",
    "\n",
    "  if http_method == \"POST\":\n",
    "    encoded_post_data = req.to_postdata()\n",
    "  else:\n",
    "    encoded_post_data = None\n",
    "    url = req.to_url()\n",
    "\n",
    "  opener = urllib.OpenerDirector()\n",
    "  opener.add_handler(http_handler)\n",
    "  opener.add_handler(https_handler)\n",
    "\n",
    "  response = opener.open(url, encoded_post_data)\n",
    "\n",
    "  return response\n",
    "\n",
    "#####get big ben clock tweets id\n",
    "def fetch_bigben():\n",
    "  url = \"https://api.twitter.com/1.1/search/tweets.json?q=big_ben_clock&count=100&result_type=recent\"\n",
    "  parameters = []\n",
    "  response = twitterreq(url, \"GET\", parameters)\n",
    "  return response\n",
    "\n",
    "id_list=[]\n",
    "re=fetch_bigben()\n",
    "re_j=json.load(re)\n",
    "re_j=re_j['statuses']\n",
    "for a in re_j:\n",
    "  id_list.append(a[\"id\"])\n",
    "#####end of big ben\n",
    "\n",
    "#####get tweets about a topic using keywords\n",
    "tweets_num=1000 # number of tweets we want\n",
    "parameters = []\n",
    "js=[]\n",
    "for i in range(tweets_num/100):\n",
    "  url = \"https://api.twitter.com/1.1/search/tweets.json?q='bitcoin''ethereum'&lang=en&count=100&result_type=recent&max_id=\"+str(id_list[2*i])\n",
    "  response = twitterreq(url, \"GET\", parameters)\n",
    "  re_j=json.load(response)\n",
    "  re_j=re_j['statuses']\n",
    "  js.append(re_j)\n",
    "  time.sleep(1)\n",
    "\n",
    "ids=[]\n",
    "screen_name=[]\n",
    "followers=[]\n",
    "listed=[]\n",
    "retweet=[]\n",
    "inreplyto=[]\n",
    "favorite=[]\n",
    "friends=[]\n",
    "text=[]\n",
    "location=[]\n",
    "\n",
    "for tweets in js:\n",
    "  for tweet in tweets:\n",
    "    if tweet.get('user'):\n",
    "        ids.append(tweet['user']['id']) \n",
    "\tretweet.append(tweet['retweet_count'])\n",
    "\tfavorite.append(tweet['favorite_count'])\n",
    "\tinreplyto.append(tweet['in_reply_to_screen_name'])\n",
    "\tfriends.append(tweet['user']['friends_count'])\n",
    "        screen_name.append(tweet['user']['screen_name'])\n",
    "        followers.append(tweet['user']['followers_count'])\n",
    "        listed.append(tweet['user']['listed_count'])\n",
    "        text.append(tweet['text'])\n",
    "        location.append(tweet['user']['location'])\n",
    "####################################################\n",
    "\n",
    "#######output tweets file\n",
    "out = open('tweets_thu.csv','wb')\n",
    "print >> out, 'ids, screen_name, followers, retweet, inreplyto, favorite, friends, listed, location, text'\n",
    "\n",
    "rows = zip(ids, screen_name, followers, retweet, inreplyto, favorite, friends, listed, location, text)\n",
    "\n",
    "csv = writer(out)\n",
    "\n",
    "for row in rows:\n",
    "    values = [(value.encode('utf8') if hasattr(value, 'encode') else value) for value in row]\n",
    "    csv.writerow(values)\n",
    "\n",
    "out.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
